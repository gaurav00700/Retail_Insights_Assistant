
## ğŸ“Š Multi-agent for Conversation and Data analytics

### Features
* LangGraph (multi-agent orchestration)
* DuckDB (in-memory SQL analytics)
* LLM reasoning (DeepSeek / Ollama / OpenAI)
* Streamlit frontend
* FastAPI backend
* Docker Compose deployment
* Local LLM serving

## ğŸ“¸ Architecture Overview
Pipeline Overview |
:-------------------------:
<img src="assets/ArchitectureDiagram.png" style="display: block; margin-left: auto; margin-right: auto; max-width: 75%; height: auto;" >

Multi-Agent Swarm |
:-------------------------:
<img src="assets/AgentSwarm.png" style="display: block; margin-left: auto; margin-right: auto; max-width: 75%; height: auto;" >

Frontend Interface |
:-------------------------:
<img src="assets/Frontend.png" style="display: block; margin-left: auto; margin-right: auto; max-width: 75%; height: auto;" >

## Agents task:
* supervisor â†’ Supervisor agent routing between workers
* data_processing â†’ Load the data and process 
* resolve_query â†’ Natural language â†’ SQL
* extract_data â†’ Executes SQL via DuckDB
* summarization â†’ Generates insights from SQL output
* conversation â†’ General chit-chat

## ğŸ“ Folder Structure
```
retail-insights/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                  # FastAPI API + streaming endpoints
â”‚   â”œâ”€â”€ backend.py              # LangGraph workflow
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py                  # Streamlit Frontend UI
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚
â””â”€â”€ docker-compose.yml          # Running pipeline
```

## ğŸš€ Run the application (Docker Compose)
Run the pipeline with following command
```bash
docker compose up --build
```

Access the services:
* Frontend (Streamlit) â†’ http://localhost:8501
* Backend (FastAPI) â†’ http://localhost:8000

## âš™ï¸ Configuration

The Streamlit sidebar allows you to:
* Input a new file path manually
* Automatically reload the chatbot with the new dataset

Others
* Changing the LLM model in backend/backend.py:
```bash
supervisor_llm = ChatOllama(model="deepseek-r1:8b", temperature=0.0)
agent_llm = ChatOllama(model="deepseek-r1:8b", temperature=0.0)
summary_llm = ChatOllama(model="deepseek-r1:8b", temperature=0.0)
conv_llm = ChatOllama(model="deepseek-r1:8b", temperature=0.7)
```